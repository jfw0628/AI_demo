{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a34227",
   "metadata": {},
   "source": [
    "First, the most import element Tensor\n",
    "Tensor is quite the same with ndarray in numpy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57376890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf61409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.8994e+14,  4.5874e-41,  1.1169e+23],\n",
      "        [ 3.0775e-41,  4.4842e-44,  0.0000e+00],\n",
      "        [ 8.9683e-44,  0.0000e+00,  1.1138e+23],\n",
      "        [ 3.0775e-41,  0.0000e+00,  1.7163e-05],\n",
      "        [ 0.0000e+00,  9.4514e-03,  0.0000e+00]]) torch.Size([5, 3])\n",
      "[[4.66041149e-310 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 4.94065646e-324]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 7.41098469e-323]\n",
      " [1.74918235e-309 1.25622152e-311 1.02765654e-321]] (5, 3)\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor(5, 3)\n",
    "n = np.empty((5, 3))\n",
    "print(t, t.shape)\n",
    "print(n, n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84947d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0543, 0.3960, 0.3582],\n",
       "        [0.7760, 0.5389, 0.1884],\n",
       "        [0.4290, 0.3583, 0.9707],\n",
       "        [0.3591, 0.1661, 0.0415],\n",
       "        [0.2520, 0.2173, 0.7167]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7777868",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe6f875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8994e+14,  3.4227e-01,  1.1169e+23],\n",
       "        [ 1.0919e-02,  3.0073e-01,  1.7690e-01],\n",
       "        [ 1.5206e-03,  7.1752e-01,  1.1138e+23],\n",
       "        [ 6.6636e-01,  6.3252e-01,  7.3975e-01],\n",
       "        [ 5.1726e-01,  8.9382e-02,  6.2015e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38753cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a1b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(2,2), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cd256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bafd33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52be103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9456889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18bcdd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "476f8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "335a6d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = torch.Tensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44adf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70a4407e-3866-4f9c-8259-267188a6d7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4521dcc2-cfd5-4576-8853-df664f91e8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b1cae37-1421-4257-8840-98db217b39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "e = np.random.randn()\n",
    "f = np.random.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8e71d4c-fd91-4021-8095-486ce9e0e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 4e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f74f9b1-20b6-4a02-ad9f-0128e42d1d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 763.9751828133012\n",
      "5099 757.5023886903538\n",
      "10099 751.0861867598618\n",
      "15099 744.7260764603152\n",
      "20099 738.4215617048901\n",
      "25099 732.1721508410533\n",
      "30099 725.9773566105089\n",
      "35099 719.8366961095446\n",
      "40099 713.7496907496449\n",
      "45099 707.7158662185958\n",
      "50099 701.7347524418744\n",
      "55099 695.8058835443524\n",
      "60099 689.9287978124073\n",
      "65099 684.1030376563908\n",
      "70099 678.3281495733854\n",
      "75099 672.6036841103125\n",
      "80099 666.9291958274387\n",
      "85099 661.3042432621145\n",
      "90099 655.7283888929356\n",
      "95099 650.2011991041583\n",
      "Result: y = -0.4439630541122855 + 1.153627950988562 x + -0.6392832735384499 x^3 + 0.13619872819004 x^5 + -0.008772475599070758 x^7\n"
     ]
    }
   ],
   "source": [
    "for t in range(100000):\n",
    "    y_pred = a + b*x + d*x**3 + e*x**5 + f*x**7\n",
    "\n",
    "    loss = np.square(y - y_pred).sum() \n",
    "    if t % 5000 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2 * (y - y_pred)\n",
    "    a_grad = grad_y_pred.sum()\n",
    "    b_grad = (grad_y_pred * x).sum()\n",
    "    d_grad = (grad_y_pred * x ** 3).sum()\n",
    "    e_grad = (grad_y_pred * x ** 5).sum()\n",
    "    f_grad = (grad_y_pred * x ** 7).sum()\n",
    "\n",
    "    a += a_grad * learning_rate\n",
    "    b += b_grad * learning_rate\n",
    "    d += d_grad * learning_rate\n",
    "    e += e_grad * learning_rate\n",
    "    f += f_grad * learning_rate\n",
    "\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {d} x^3 + {e} x^5 + {f} x^7')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "223a6b62-1d15-40b7-9fd0-8ae1e70c9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu')\n",
    "\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fc38dd27-178b-4021-84cb-cfec8e423180",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "37fbbd4c-9c23-467b-b996-5a8e656a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "e = torch.randn((), device=device, dtype=dtype)\n",
    "f = torch.randn((), device=device, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f7f262d-436a-4194-9a41-e6ce01f2b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 342222.0314785214\n",
      "5099 13936.008191303183\n",
      "10099 12472.592888638937\n",
      "15099 11174.972430907685\n",
      "20099 10024.163203668544\n",
      "25099 9003.35842111805\n",
      "30099 8097.678461016789\n",
      "35099 7293.949835035301\n",
      "40099 6580.5095101570205\n",
      "45099 5947.03167348092\n",
      "50099 5384.37436625868\n",
      "55099 4884.4437082509985\n",
      "60099 4440.073694868043\n",
      "65099 4044.919780961708\n",
      "70099 3693.3646700003073\n",
      "75099 3380.4349087204328\n",
      "80099 3101.7270479145627\n",
      "85099 2853.342272159801\n",
      "90099 2631.82852713739\n",
      "95099 2434.129284602057\n",
      "100099 2257.538183691864\n",
      "105099 2099.6588745882837\n",
      "110099 1958.3694678405514\n",
      "115099 1831.7910611047687\n",
      "120099 1718.259875637089\n",
      "125099 1616.3025885187533\n",
      "130099 1524.6144940771678\n",
      "135099 1442.0401700076077\n",
      "140099 1367.5563609180736\n",
      "145099 1300.25682496991\n",
      "150099 1239.3389184561772\n",
      "155099 1184.0917189853253\n",
      "160099 1133.8855107997865\n",
      "165099 1088.1624759996776\n",
      "170099 1046.4284533604673\n",
      "175099 1008.2456422977064\n",
      "180099 973.2261435752812\n",
      "185099 941.0262407877226\n",
      "190099 911.3413376541074\n",
      "195099 883.9014759057283\n",
      "200099 858.467367177406\n",
      "205099 834.8268799492375\n",
      "210099 812.7919293477855\n",
      "215099 792.1957236014075\n",
      "220099 772.8903262443955\n",
      "225099 754.7444978556334\n",
      "230099 737.6417852716872\n",
      "235099 721.4788298910522\n",
      "240099 706.1638699418393\n",
      "245099 691.6154144670527\n",
      "250099 677.7610693333511\n",
      "255099 664.5364978277617\n",
      "260099 651.8845004067211\n",
      "265099 639.7541999321503\n",
      "270099 628.1003202966302\n",
      "275099 616.8825477273052\n",
      "280099 606.0649652865745\n",
      "285099 595.6155521750534\n",
      "290099 585.5057404053285\n",
      "295099 575.7100222670795\n",
      "300099 566.2056027589351\n",
      "305099 556.9720918305838\n",
      "310099 547.991231869644\n",
      "315099 539.2466563920391\n",
      "320099 530.7236763575405\n",
      "325099 522.4090909430184\n",
      "330099 514.2910199688389\n",
      "335099 506.35875549579106\n",
      "340099 498.6026303945339\n",
      "345099 491.01390194158375\n",
      "350099 483.58464871925844\n",
      "355099 476.3076792942714\n",
      "360099 469.17645132485075\n",
      "365099 462.1849999009533\n",
      "370099 455.32787405931606\n",
      "375099 448.60008053640263\n",
      "380099 441.9970339298077\n",
      "385099 435.5145125337257\n",
      "390099 429.14861919844475\n",
      "395099 422.8957466382059\n",
      "400099 416.7525466779098\n",
      "405099 410.71590298753324\n",
      "410099 404.78290690487637\n",
      "415099 398.9508359929779\n",
      "420099 393.21713501923796\n",
      "425099 387.57939907900095\n",
      "430099 382.0353586183221\n",
      "435099 376.5828661385482\n",
      "440099 371.21988439052075\n",
      "445099 365.94447588799454\n",
      "450099 360.7547935895843\n",
      "455099 355.64907261573273\n",
      "460099 350.6256228825346\n",
      "465099 345.6828225477583\n",
      "470099 340.81911217645586\n",
      "475099 336.03298954413776\n",
      "480099 331.3230050048553\n",
      "485099 326.6877573599271\n",
      "490099 322.1258901703676\n",
      "495099 317.63608846259444\n",
      "Result: y = -0.36500627096844646 + 0.6190398436762273 x + -0.020096165603328168 x^3 + -0.005142055643357288 x^5\n"
     ]
    }
   ],
   "source": [
    "for t in range(500000):\n",
    "    y_pred = a + b*x + d*x**3 + e*x**5\n",
    "\n",
    "    loss = (y - y_pred).pow(2).sum().item()\n",
    "    if t % 5000 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2 * (y - y_pred)\n",
    "    a_grad = grad_y_pred.sum()\n",
    "    b_grad = (grad_y_pred * x).sum()\n",
    "    d_grad = (grad_y_pred * x ** 3).sum()\n",
    "    e_grad = (grad_y_pred * x ** 5).sum()\n",
    "\n",
    "    a += a_grad * learning_rate\n",
    "    b += b_grad * learning_rate\n",
    "    d += d_grad * learning_rate\n",
    "    e += e_grad * learning_rate\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {d.item()} x^3 + {e.item()} x^5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63dd99d0-f8fc-4550-85c9-75f96847e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 383427.40625\n",
      "10099 391310.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Use autograd to compute the backward pass. This call will compute the\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# gradient of loss with respect to all Tensors with requires_grad=True.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# the gradient of the loss with respect to a, b, c, d respectively.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Manually update weights using gradient descent. Wrap in torch.no_grad()\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# because weights have requires_grad=True, but we don't need to track this\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# in autograd.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/opencv/lib/python3.11/site-packages/torch/_tensor.py:478\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/opencv/lib/python3.11/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/opencv/lib/python3.11/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opencv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opencv/lib/python3.11/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/opencv/lib/python3.11/site-packages/torch/autograd/__init__.py:89\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "e = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-7\n",
    "for t in range(100000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + d * x ** 3 + e * x ** 5\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 10000 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        d -= learning_rate * e.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        e.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {d.item()} x^3 + {e.item()} x^5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "99456724-2e48-4bbd-9513-271e37351869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.1416,   9.8696, -31.0063],\n",
       "        [ -3.1384,   9.8499, -30.9133],\n",
       "        [ -3.1353,   9.8301, -30.8205],\n",
       "        ...,\n",
       "        [  3.1353,   9.8301,  30.8205],\n",
       "        [  3.1384,   9.8499,  30.9133],\n",
       "        [  3.1416,   9.8696,  31.0063]], device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "48ddb3b7-0ead-4350-ae69-f8e18e0d8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2087.344970703125\n",
      "199 1574.2977294921875\n",
      "299 1345.9932861328125\n",
      "399 1129.1258544921875\n",
      "499 931.81884765625\n",
      "599 756.5601196289062\n",
      "699 602.7662353515625\n",
      "799 469.18951416015625\n",
      "899 354.90826416015625\n",
      "999 259.009033203125\n",
      "1099 180.67726135253906\n",
      "1199 119.09180450439453\n",
      "1299 73.21722412109375\n",
      "1399 41.67363739013672\n",
      "1499 22.522953033447266\n",
      "1599 12.864471435546875\n",
      "1699 9.52615737915039\n",
      "1799 8.93216323852539\n",
      "1899 8.926095962524414\n",
      "1999 8.925756454467773\n",
      "Result: y = 0.0005015641218051314 + 0.8570855855941772 x + 0.0005015628412365913 x^2 + -0.09279617667198181 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "580145e8-aa21-4117-b8ff-ab75d89ea136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 5085.8916015625\n",
      "199 3439.41845703125\n",
      "299 2329.237548828125\n",
      "399 1579.9462890625\n",
      "499 1073.7333984375\n",
      "599 731.3973388671875\n",
      "699 499.6471252441406\n",
      "799 342.5948486328125\n",
      "899 236.04977416992188\n",
      "999 163.68997192382812\n",
      "1099 114.492919921875\n",
      "1199 81.00645446777344\n",
      "1299 58.18792724609375\n",
      "1399 42.620994567871094\n",
      "1499 31.98900604248047\n",
      "1599 24.71915626525879\n",
      "1699 19.742523193359375\n",
      "1799 16.331775665283203\n",
      "1899 13.991594314575195\n",
      "1999 12.384117126464844\n",
      "2099 11.278669357299805\n",
      "2199 10.51762580871582\n",
      "2299 9.993104934692383\n",
      "2399 9.631206512451172\n",
      "2499 9.38125228881836\n",
      "2599 9.208422660827637\n",
      "2699 9.088798522949219\n",
      "2799 9.0059232711792\n",
      "2899 8.948451042175293\n",
      "2999 8.90855598449707\n",
      "3099 8.880836486816406\n",
      "3199 8.861560821533203\n",
      "3299 8.84814453125\n",
      "3399 8.838798522949219\n",
      "3499 8.832283973693848\n",
      "3599 8.827737808227539\n",
      "3699 8.824563980102539\n",
      "3799 8.822344779968262\n",
      "3899 8.820795059204102\n",
      "3999 8.819709777832031\n",
      "4099 8.818950653076172\n",
      "4199 8.818418502807617\n",
      "4299 8.818045616149902\n",
      "4399 8.817784309387207\n",
      "4499 8.817601203918457\n",
      "4599 8.817471504211426\n",
      "4699 8.817380905151367\n",
      "4799 8.817317962646484\n",
      "4899 8.817273139953613\n",
      "4999 8.817241668701172\n",
      "5099 8.817218780517578\n",
      "5199 8.817203521728516\n",
      "5299 8.817192077636719\n",
      "5399 8.817185401916504\n",
      "5499 8.817179679870605\n",
      "5599 8.817174911499023\n",
      "5699 8.81717300415039\n",
      "5799 8.817171096801758\n",
      "5899 8.817169189453125\n",
      "5999 8.817168235778809\n",
      "6099 8.817168235778809\n",
      "6199 8.817167282104492\n",
      "6299 8.817167282104492\n",
      "6399 8.817167282104492\n",
      "6499 8.817167282104492\n",
      "6599 8.817166328430176\n",
      "6699 8.817167282104492\n",
      "6799 8.817166328430176\n",
      "6899 8.817166328430176\n",
      "6999 8.817167282104492\n",
      "7099 8.81716537475586\n",
      "7199 8.817166328430176\n",
      "7299 8.817167282104492\n",
      "7399 8.817167282104492\n",
      "7499 8.817166328430176\n",
      "7599 8.817167282104492\n",
      "7699 8.817166328430176\n",
      "7799 8.817167282104492\n",
      "7899 8.817166328430176\n",
      "7999 8.817166328430176\n",
      "8099 8.817166328430176\n",
      "8199 8.817166328430176\n",
      "8299 8.817167282104492\n",
      "8399 8.817167282104492\n",
      "8499 8.817166328430176\n",
      "8599 8.817166328430176\n",
      "8699 8.817166328430176\n",
      "8799 8.817166328430176\n",
      "8899 8.817166328430176\n",
      "8999 8.817166328430176\n",
      "9099 8.817166328430176\n",
      "9199 8.817166328430176\n",
      "9299 8.817166328430176\n",
      "9399 8.817166328430176\n",
      "9499 8.817166328430176\n",
      "9599 8.817166328430176\n",
      "9699 8.817166328430176\n",
      "9799 8.817166328430176\n",
      "9899 8.817166328430176\n",
      "9999 8.817166328430176\n",
      "10099 8.81716537475586\n",
      "10199 8.81716537475586\n",
      "10299 8.817166328430176\n",
      "10399 8.817166328430176\n",
      "10499 8.817166328430176\n",
      "10599 8.817166328430176\n",
      "10699 8.817166328430176\n",
      "10799 8.817166328430176\n",
      "10899 8.817166328430176\n",
      "10999 8.817166328430176\n",
      "11099 8.817166328430176\n",
      "11199 8.817166328430176\n",
      "11299 8.817166328430176\n",
      "11399 8.817166328430176\n",
      "11499 8.817166328430176\n",
      "11599 8.817166328430176\n",
      "11699 8.817166328430176\n",
      "11799 8.817166328430176\n",
      "11899 8.817166328430176\n",
      "11999 8.817166328430176\n",
      "12099 8.817166328430176\n",
      "12199 8.817166328430176\n",
      "12299 8.817166328430176\n",
      "12399 8.817166328430176\n",
      "12499 8.817166328430176\n",
      "12599 8.817166328430176\n",
      "12699 8.817166328430176\n",
      "12799 8.817166328430176\n",
      "12899 8.817166328430176\n",
      "12999 8.817166328430176\n",
      "13099 8.817166328430176\n",
      "13199 8.817166328430176\n",
      "13299 8.817166328430176\n",
      "13399 8.817166328430176\n",
      "13499 8.817166328430176\n",
      "13599 8.817166328430176\n",
      "13699 8.817166328430176\n",
      "13799 8.817166328430176\n",
      "13899 8.817166328430176\n",
      "13999 8.817166328430176\n",
      "14099 8.817166328430176\n",
      "14199 8.817166328430176\n",
      "14299 8.817166328430176\n",
      "14399 8.817166328430176\n",
      "14499 8.817166328430176\n",
      "14599 8.817166328430176\n",
      "14699 8.817166328430176\n",
      "14799 8.817166328430176\n",
      "14899 8.817166328430176\n",
      "14999 8.817166328430176\n",
      "15099 8.817166328430176\n",
      "15199 8.817166328430176\n",
      "15299 8.817166328430176\n",
      "15399 8.817166328430176\n",
      "15499 8.817166328430176\n",
      "15599 8.817166328430176\n",
      "15699 8.817166328430176\n",
      "15799 8.817166328430176\n",
      "15899 8.817166328430176\n",
      "15999 8.817166328430176\n",
      "16099 8.817166328430176\n",
      "16199 8.817166328430176\n",
      "16299 8.817166328430176\n",
      "16399 8.817166328430176\n",
      "16499 8.817166328430176\n",
      "16599 8.817166328430176\n",
      "16699 8.817166328430176\n",
      "16799 8.817166328430176\n",
      "16899 8.817166328430176\n",
      "16999 8.817166328430176\n",
      "17099 8.817166328430176\n",
      "17199 8.817166328430176\n",
      "17299 8.817166328430176\n",
      "17399 8.817166328430176\n",
      "17499 8.817166328430176\n",
      "17599 8.817166328430176\n",
      "17699 8.817166328430176\n",
      "17799 8.817166328430176\n",
      "17899 8.817166328430176\n",
      "17999 8.817166328430176\n",
      "18099 8.817166328430176\n",
      "18199 8.817166328430176\n",
      "18299 8.817166328430176\n",
      "18399 8.817166328430176\n",
      "18499 8.817166328430176\n",
      "18599 8.817166328430176\n",
      "18699 8.817166328430176\n",
      "18799 8.817166328430176\n",
      "18899 8.817166328430176\n",
      "18999 8.817166328430176\n",
      "19099 8.817166328430176\n",
      "19199 8.817166328430176\n",
      "19299 8.817166328430176\n",
      "19399 8.817166328430176\n",
      "19499 8.817166328430176\n",
      "19599 8.817166328430176\n",
      "19699 8.817166328430176\n",
      "19799 8.817166328430176\n",
      "19899 8.817166328430176\n",
      "19999 8.817166328430176\n",
      "Result: y = 1.3759663408663414e-09 + 0.8567265868186951 x + 1.0967430519315258e-08 x^2 + -0.09332836419343948 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(20000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e51ad47a-c86a-4b5d-b175-6a3da4b5cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 3376.126953125\n",
      "3999 1523.5103759765625\n",
      "5999 737.3735961914062\n",
      "7999 319.46978759765625\n",
      "9999 148.5950164794922\n",
      "11999 75.62993621826172\n",
      "13999 38.52421188354492\n",
      "15999 22.107807159423828\n",
      "17999 14.873477935791016\n",
      "19999 11.55041217803955\n",
      "21999 10.043790817260742\n",
      "23999 9.381021499633789\n",
      "25999 8.931184768676758\n",
      "27999 8.932641983032227\n",
      "29999 8.922027587890625\n",
      "Result: y = -0.004001822788268328 + 0.8514901399612427 x + 0.0001422598579665646 x^2 + -0.0929984301328659 x^3 + 0.00010318189015379176 x^4 ? + 0.00010318189015379176 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e64e-f6bf-46ee-960c-ad84176d5da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
