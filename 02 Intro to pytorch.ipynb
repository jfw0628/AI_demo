{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a34227",
   "metadata": {},
   "source": [
    "First, the most import element Tensor\n",
    "Tensor is quite the same with ndarray in numpy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57376890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf61409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5785e+18,  4.5841e-41,  1.5785e+18],\n",
      "        [ 4.5841e-41,  1.0413e+09, -9.7947e-20],\n",
      "        [-8.1188e-32,  4.5839e-41, -4.7726e-32],\n",
      "        [ 4.5839e-41, -2.4910e-23,  1.1031e-37],\n",
      "        [-8.1223e-32,  4.5839e-41, -4.7724e-32]]) torch.Size([5, 3])\n",
      "[[2.8661412e-316 0.0000000e+000 0.0000000e+000]\n",
      " [0.0000000e+000 0.0000000e+000 0.0000000e+000]\n",
      " [0.0000000e+000 0.0000000e+000 0.0000000e+000]\n",
      " [0.0000000e+000 0.0000000e+000 0.0000000e+000]\n",
      " [0.0000000e+000 0.0000000e+000 0.0000000e+000]] (5, 3)\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor(5, 3)\n",
    "n = np.empty((5, 3))\n",
    "print(t, t.shape)\n",
    "print(n, n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84947d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2028, 0.6863, 0.2941],\n",
       "        [0.3226, 0.6021, 0.9086],\n",
       "        [0.8982, 0.1444, 0.7632],\n",
       "        [0.1940, 0.0420, 0.8208],\n",
       "        [0.0470, 0.9708, 0.6122]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7777868",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe6f875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5785e+18, 9.9412e-01, 1.5785e+18],\n",
       "        [7.7773e-01, 1.0413e+09, 2.3234e-02],\n",
       "        [7.2017e-01, 3.6664e-01, 4.6233e-01],\n",
       "        [9.8669e-01, 1.0943e-01, 6.5858e-02],\n",
       "        [6.8959e-01, 4.8997e-01, 2.0376e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38753cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a1b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(2,2), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafd33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52be103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9456889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18bcdd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "476f8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.norm() < 1000:\n",
    "    y = y * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "335a6d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = torch.Tensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4521dcc2-cfd5-4576-8853-df664f91e8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "26931a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "d = np.random.randn()\n",
    "e = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f74f9b1-20b6-4a02-ad9f-0128e42d1d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2906.0783443431374\n",
      "599 192.91712250469027\n",
      "1099 28.78169988382963\n",
      "1599 11.2810226767477\n",
      "2099 9.127563862307282\n",
      "2599 8.856387435105598\n",
      "3099 8.82212366288763\n",
      "3599 8.817792246667247\n",
      "4099 8.817244657218932\n",
      "4599 8.817175428771202\n",
      "5099 8.817166676621355\n",
      "5599 8.817165570137742\n",
      "6099 8.817165430251425\n",
      "6599 8.817165412566407\n",
      "7099 8.817165410330592\n",
      "7599 8.81716541004793\n",
      "8099 8.817165410012198\n",
      "8599 8.817165410007679\n",
      "9099 8.817165410007107\n",
      "9599 8.817165410007036\n",
      "Result: y = -4.150906960851466e-17 + 0.8567408416997583 x + -0.09333038884515558 x^3 + -0.9335781556814468 x^5 \n"
     ]
    }
   ],
   "source": [
    "min_loss = np.inf\n",
    "\n",
    "\n",
    "for t in range(10000):\n",
    "    y_pred = a + b*x + d*x**3 \n",
    "\n",
    "    loss = np.square(y - y_pred).sum() \n",
    "    if abs(min_loss - loss) < learning_rate:\n",
    "        break\n",
    "        \n",
    "    if loss < min_loss:\n",
    "        min_loss = np.inf\n",
    "    \n",
    "    if t % 500 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2 * (y - y_pred)\n",
    "    a_grad = grad_y_pred.sum()\n",
    "    b_grad = (grad_y_pred * x).sum()\n",
    "    d_grad = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "\n",
    "    a += a_grad * learning_rate\n",
    "    b += b_grad * learning_rate\n",
    "    d += d_grad * learning_rate\n",
    "   \n",
    "\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {d} x^3 + {e} x^5 ')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8e71d4c-fd91-4021-8095-486ce9e0e53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "223a6b62-1d15-40b7-9fd0-8ae1e70c9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu')\n",
    "\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc38dd27-178b-4021-84cb-cfec8e423180",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "37fbbd4c-9c23-467b-b996-5a8e656a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1f7f262d-436a-4194-9a41-e6ce01f2b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 15.683304604030335\n",
      "Result: y = 5.570931458566834e-10 + 0.856715582089617 x + -0.0933267958855756 x^3\n"
     ]
    }
   ],
   "source": [
    "for t in range(4000):\n",
    "    y_pred = a + b*x + d*x**3 \n",
    "\n",
    "    loss = (y - y_pred).pow(2).sum().item()\n",
    "    if t % 5000 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2 * (y - y_pred)\n",
    "    a_grad = grad_y_pred.sum()\n",
    "    b_grad = (grad_y_pred * x).sum()\n",
    "    d_grad = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    a += a_grad * learning_rate\n",
    "    b += b_grad * learning_rate\n",
    "    d += d_grad * learning_rate\n",
    "   \n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "63dd99d0-f8fc-4550-85c9-75f96847e8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1080\n",
      "99 286951.21875\n",
      "10099 283314.84375\n",
      "20099 283313.21875\n",
      "30099 283313.15625\n",
      "40099 283313.1875\n",
      "50099 283313.1875\n",
      "60099 283313.1875\n",
      "70099 283313.1875\n",
      "80099 283313.1875\n",
      "90099 283313.15625\n",
      "Result: y = 2.406015084943647e-07 + 27.11090660095215 x + -11.561665534973145 x^3 + 0.9975190758705139 x^5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # You can specify the GPU index\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "e = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-7\n",
    "for t in range(100000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + d * x ** 3 + e * x ** 5\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 10000 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        d -= learning_rate * e.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        e.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {d.item()} x^3 + {e.item()} x^5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0654c58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c6375dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_register_device_module', 'device', 'get_device']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(torch) if 'device' in  i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "99456724-2e48-4bbd-9513-271e37351869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.1416,   9.8696, -31.0063],\n",
       "        [ -3.1384,   9.8499, -30.9133],\n",
       "        [ -3.1353,   9.8301, -30.8205],\n",
       "        ...,\n",
       "        [  3.1353,   9.8301,  30.8205],\n",
       "        [  3.1384,   9.8499,  30.9133],\n",
       "        [  3.1416,   9.8696,  31.0063]], device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "48ddb3b7-0ead-4350-ae69-f8e18e0d8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2087.344970703125\n",
      "199 1574.2977294921875\n",
      "299 1345.9932861328125\n",
      "399 1129.1258544921875\n",
      "499 931.81884765625\n",
      "599 756.5601196289062\n",
      "699 602.7662353515625\n",
      "799 469.18951416015625\n",
      "899 354.90826416015625\n",
      "999 259.009033203125\n",
      "1099 180.67726135253906\n",
      "1199 119.09180450439453\n",
      "1299 73.21722412109375\n",
      "1399 41.67363739013672\n",
      "1499 22.522953033447266\n",
      "1599 12.864471435546875\n",
      "1699 9.52615737915039\n",
      "1799 8.93216323852539\n",
      "1899 8.926095962524414\n",
      "1999 8.925756454467773\n",
      "Result: y = 0.0005015641218051314 + 0.8570855855941772 x + 0.0005015628412365913 x^2 + -0.09279617667198181 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "580145e8-aa21-4117-b8ff-ab75d89ea136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 5085.8916015625\n",
      "199 3439.41845703125\n",
      "299 2329.237548828125\n",
      "399 1579.9462890625\n",
      "499 1073.7333984375\n",
      "599 731.3973388671875\n",
      "699 499.6471252441406\n",
      "799 342.5948486328125\n",
      "899 236.04977416992188\n",
      "999 163.68997192382812\n",
      "1099 114.492919921875\n",
      "1199 81.00645446777344\n",
      "1299 58.18792724609375\n",
      "1399 42.620994567871094\n",
      "1499 31.98900604248047\n",
      "1599 24.71915626525879\n",
      "1699 19.742523193359375\n",
      "1799 16.331775665283203\n",
      "1899 13.991594314575195\n",
      "1999 12.384117126464844\n",
      "2099 11.278669357299805\n",
      "2199 10.51762580871582\n",
      "2299 9.993104934692383\n",
      "2399 9.631206512451172\n",
      "2499 9.38125228881836\n",
      "2599 9.208422660827637\n",
      "2699 9.088798522949219\n",
      "2799 9.0059232711792\n",
      "2899 8.948451042175293\n",
      "2999 8.90855598449707\n",
      "3099 8.880836486816406\n",
      "3199 8.861560821533203\n",
      "3299 8.84814453125\n",
      "3399 8.838798522949219\n",
      "3499 8.832283973693848\n",
      "3599 8.827737808227539\n",
      "3699 8.824563980102539\n",
      "3799 8.822344779968262\n",
      "3899 8.820795059204102\n",
      "3999 8.819709777832031\n",
      "4099 8.818950653076172\n",
      "4199 8.818418502807617\n",
      "4299 8.818045616149902\n",
      "4399 8.817784309387207\n",
      "4499 8.817601203918457\n",
      "4599 8.817471504211426\n",
      "4699 8.817380905151367\n",
      "4799 8.817317962646484\n",
      "4899 8.817273139953613\n",
      "4999 8.817241668701172\n",
      "5099 8.817218780517578\n",
      "5199 8.817203521728516\n",
      "5299 8.817192077636719\n",
      "5399 8.817185401916504\n",
      "5499 8.817179679870605\n",
      "5599 8.817174911499023\n",
      "5699 8.81717300415039\n",
      "5799 8.817171096801758\n",
      "5899 8.817169189453125\n",
      "5999 8.817168235778809\n",
      "6099 8.817168235778809\n",
      "6199 8.817167282104492\n",
      "6299 8.817167282104492\n",
      "6399 8.817167282104492\n",
      "6499 8.817167282104492\n",
      "6599 8.817166328430176\n",
      "6699 8.817167282104492\n",
      "6799 8.817166328430176\n",
      "6899 8.817166328430176\n",
      "6999 8.817167282104492\n",
      "7099 8.81716537475586\n",
      "7199 8.817166328430176\n",
      "7299 8.817167282104492\n",
      "7399 8.817167282104492\n",
      "7499 8.817166328430176\n",
      "7599 8.817167282104492\n",
      "7699 8.817166328430176\n",
      "7799 8.817167282104492\n",
      "7899 8.817166328430176\n",
      "7999 8.817166328430176\n",
      "8099 8.817166328430176\n",
      "8199 8.817166328430176\n",
      "8299 8.817167282104492\n",
      "8399 8.817167282104492\n",
      "8499 8.817166328430176\n",
      "8599 8.817166328430176\n",
      "8699 8.817166328430176\n",
      "8799 8.817166328430176\n",
      "8899 8.817166328430176\n",
      "8999 8.817166328430176\n",
      "9099 8.817166328430176\n",
      "9199 8.817166328430176\n",
      "9299 8.817166328430176\n",
      "9399 8.817166328430176\n",
      "9499 8.817166328430176\n",
      "9599 8.817166328430176\n",
      "9699 8.817166328430176\n",
      "9799 8.817166328430176\n",
      "9899 8.817166328430176\n",
      "9999 8.817166328430176\n",
      "10099 8.81716537475586\n",
      "10199 8.81716537475586\n",
      "10299 8.817166328430176\n",
      "10399 8.817166328430176\n",
      "10499 8.817166328430176\n",
      "10599 8.817166328430176\n",
      "10699 8.817166328430176\n",
      "10799 8.817166328430176\n",
      "10899 8.817166328430176\n",
      "10999 8.817166328430176\n",
      "11099 8.817166328430176\n",
      "11199 8.817166328430176\n",
      "11299 8.817166328430176\n",
      "11399 8.817166328430176\n",
      "11499 8.817166328430176\n",
      "11599 8.817166328430176\n",
      "11699 8.817166328430176\n",
      "11799 8.817166328430176\n",
      "11899 8.817166328430176\n",
      "11999 8.817166328430176\n",
      "12099 8.817166328430176\n",
      "12199 8.817166328430176\n",
      "12299 8.817166328430176\n",
      "12399 8.817166328430176\n",
      "12499 8.817166328430176\n",
      "12599 8.817166328430176\n",
      "12699 8.817166328430176\n",
      "12799 8.817166328430176\n",
      "12899 8.817166328430176\n",
      "12999 8.817166328430176\n",
      "13099 8.817166328430176\n",
      "13199 8.817166328430176\n",
      "13299 8.817166328430176\n",
      "13399 8.817166328430176\n",
      "13499 8.817166328430176\n",
      "13599 8.817166328430176\n",
      "13699 8.817166328430176\n",
      "13799 8.817166328430176\n",
      "13899 8.817166328430176\n",
      "13999 8.817166328430176\n",
      "14099 8.817166328430176\n",
      "14199 8.817166328430176\n",
      "14299 8.817166328430176\n",
      "14399 8.817166328430176\n",
      "14499 8.817166328430176\n",
      "14599 8.817166328430176\n",
      "14699 8.817166328430176\n",
      "14799 8.817166328430176\n",
      "14899 8.817166328430176\n",
      "14999 8.817166328430176\n",
      "15099 8.817166328430176\n",
      "15199 8.817166328430176\n",
      "15299 8.817166328430176\n",
      "15399 8.817166328430176\n",
      "15499 8.817166328430176\n",
      "15599 8.817166328430176\n",
      "15699 8.817166328430176\n",
      "15799 8.817166328430176\n",
      "15899 8.817166328430176\n",
      "15999 8.817166328430176\n",
      "16099 8.817166328430176\n",
      "16199 8.817166328430176\n",
      "16299 8.817166328430176\n",
      "16399 8.817166328430176\n",
      "16499 8.817166328430176\n",
      "16599 8.817166328430176\n",
      "16699 8.817166328430176\n",
      "16799 8.817166328430176\n",
      "16899 8.817166328430176\n",
      "16999 8.817166328430176\n",
      "17099 8.817166328430176\n",
      "17199 8.817166328430176\n",
      "17299 8.817166328430176\n",
      "17399 8.817166328430176\n",
      "17499 8.817166328430176\n",
      "17599 8.817166328430176\n",
      "17699 8.817166328430176\n",
      "17799 8.817166328430176\n",
      "17899 8.817166328430176\n",
      "17999 8.817166328430176\n",
      "18099 8.817166328430176\n",
      "18199 8.817166328430176\n",
      "18299 8.817166328430176\n",
      "18399 8.817166328430176\n",
      "18499 8.817166328430176\n",
      "18599 8.817166328430176\n",
      "18699 8.817166328430176\n",
      "18799 8.817166328430176\n",
      "18899 8.817166328430176\n",
      "18999 8.817166328430176\n",
      "19099 8.817166328430176\n",
      "19199 8.817166328430176\n",
      "19299 8.817166328430176\n",
      "19399 8.817166328430176\n",
      "19499 8.817166328430176\n",
      "19599 8.817166328430176\n",
      "19699 8.817166328430176\n",
      "19799 8.817166328430176\n",
      "19899 8.817166328430176\n",
      "19999 8.817166328430176\n",
      "Result: y = 1.3759663408663414e-09 + 0.8567265868186951 x + 1.0967430519315258e-08 x^2 + -0.09332836419343948 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(20000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e51ad47a-c86a-4b5d-b175-6a3da4b5cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 3376.126953125\n",
      "3999 1523.5103759765625\n",
      "5999 737.3735961914062\n",
      "7999 319.46978759765625\n",
      "9999 148.5950164794922\n",
      "11999 75.62993621826172\n",
      "13999 38.52421188354492\n",
      "15999 22.107807159423828\n",
      "17999 14.873477935791016\n",
      "19999 11.55041217803955\n",
      "21999 10.043790817260742\n",
      "23999 9.381021499633789\n",
      "25999 8.931184768676758\n",
      "27999 8.932641983032227\n",
      "29999 8.922027587890625\n",
      "Result: y = -0.004001822788268328 + 0.8514901399612427 x + 0.0001422598579665646 x^2 + -0.0929984301328659 x^3 + 0.00010318189015379176 x^4 ? + 0.00010318189015379176 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e64e-f6bf-46ee-960c-ad84176d5da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
