{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e1f8e48-58d6-4cc5-8997-ff25b15aa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3838c148-d3bd-4f44-bb1f-31b338c409b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d071a329-b10d-497c-aa1f-f05fbb7de715",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3fa2b38c-ce0e-434a-b503-6842b450e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "        print(self.flatten)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        return logits\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8cf0d4aa-5f5e-4282-8ec4-f1fa186659d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim=1, end_dim=-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "04b8fa83-3c65-4e49-adce-1dc760f5991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "693428fd-0a85-4814-b82b-c3776a2496f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7], device='cuda:0')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_probab\n",
    "y_pred = pred_probab.argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e29aac62-2dfb-49d3-8945-13df3293d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4845b062-6d95-4326-86b8-445d6e79b873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_image = nn.Flatten()(input_image)\n",
    "flatten_image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0640907-c20e-456a-a9b7-727a9c1d1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(28 * 28, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3dd6bfa-021f-4a54-9cc7-75f83ba26cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden= layer(flatten_image)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "090d438a-47ed-4067-bc95-51d8790cb977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before relu tensor([[-0.2456,  0.1949, -0.0324, -0.2991, -0.5780],\n",
      "        [-0.2370,  0.7251, -0.0850, -0.3289, -0.2571],\n",
      "        [ 0.0226,  0.2963, -0.0331, -0.4357, -0.3695]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "after relu tensor([[0.0000, 0.1949, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7251, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0226, 0.2963, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('before relu', hidden)\n",
    "print('after relu', nn.ReLU()(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9d400ca-196e-42bc-bf33-2133a3ecbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(5)\n",
    "\n",
    "w = torch.randn((5, 3), requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = torch.zeros(3)\n",
    "\n",
    "z = torch.matmul(input, w) + b\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a6976169-7b2f-4955-a60b-d47637b29ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.], device='cuda:0'),\n",
       " tensor([0., 0., 0.], device='cuda:0'),\n",
       " tensor([[-1.0013,  1.0017,  1.1053],\n",
       "         [ 2.9527,  0.5332,  1.4619],\n",
       "         [-1.2628, -0.1155,  1.2776],\n",
       "         [ 0.6971,  0.8375,  1.6511],\n",
       "         [ 2.1808, -0.9795,  0.4869]], device='cuda:0', requires_grad=True),\n",
       " tensor([ 1.3432,  0.5157, -1.3781], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, y, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f7a174f2-369a-4e22-948f-6a9475150f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f4d07eb3c10>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "239ae381-8cc5-408c-8d39-457403671a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BinaryCrossEntropyWithLogitsBackward0 at 0x7f4d07eb0100>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c16cf8ed-313d-4c39-a9e0-216a8b3d9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b1f58c15-78c2-4a50-b4f6-c9d7184003f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3309, 0.2858, 0.3300],\n",
      "        [0.3309, 0.2858, 0.3300],\n",
      "        [0.3309, 0.2858, 0.3300],\n",
      "        [0.3309, 0.2858, 0.3300],\n",
      "        [0.3309, 0.2858, 0.3300]], device='cuda:0')\n",
      "tensor([0.3309, 0.2858, 0.3300], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0801df9a-4859-479c-8116-02b4a83824a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b25e9532-4edd-499b-8471-e3ea350faa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "765d02b7-e435-4dc3-8573-4288077ee23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_datasets.data.to(device)\n",
    "test_datasets = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    download=True,\n",
    "    train=False, \n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_datasets, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "05bd25f7-a171-40e9-aebf-cb53a1af788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "47d860ac-ac84-4bab-992c-d06e91b6f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bd57cb23-5181-4125-a8b2-37d34c78cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, model, loss_fn, optimizer):\n",
    "    size = len(train_loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9159a467-d5f5-42d7-a87d-9dd64c445919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "     # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e129fb0f-d534-419f-b544-1219f754f5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "140e4c74-e256-4ce4-b66f-5265b31bc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.782308  [  512/60000]\n",
      "loss: 0.791410  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.793590 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.779005  [  512/60000]\n",
      "loss: 0.788441  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.790651 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.775759  [  512/60000]\n",
      "loss: 0.785532  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.787764 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.772573  [  512/60000]\n",
      "loss: 0.782673  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.784924 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.769437  [  512/60000]\n",
      "loss: 0.779875  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.782132 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.766349  [  512/60000]\n",
      "loss: 0.777129  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.779386 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.763308  [  512/60000]\n",
      "loss: 0.774428  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.776682 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.760314  [  512/60000]\n",
      "loss: 0.771777  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.774019 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.757368  [  512/60000]\n",
      "loss: 0.769175  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.771396 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.754467  [  512/60000]\n",
      "loss: 0.766616  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.768812 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.751610  [  512/60000]\n",
      "loss: 0.764098  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.766266 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.748794  [  512/60000]\n",
      "loss: 0.761617  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.763756 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.746019  [  512/60000]\n",
      "loss: 0.759182  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.761281 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.743284  [  512/60000]\n",
      "loss: 0.756782  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.758840 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.740587  [  512/60000]\n",
      "loss: 0.754420  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.756431 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.737925  [  512/60000]\n",
      "loss: 0.752086  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.754053 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.735300  [  512/60000]\n",
      "loss: 0.749787  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.751705 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.732707  [  512/60000]\n",
      "loss: 0.747516  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.749386 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.730142  [  512/60000]\n",
      "loss: 0.745271  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.747095 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.727609  [  512/60000]\n",
      "loss: 0.743056  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.744830 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.725107  [  512/60000]\n",
      "loss: 0.740869  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.742591 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.722634  [  512/60000]\n",
      "loss: 0.738706  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.740376 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.720187  [  512/60000]\n",
      "loss: 0.736570  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.738185 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.717768  [  512/60000]\n",
      "loss: 0.734458  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.736017 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.715375  [  512/60000]\n",
      "loss: 0.732367  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.733871 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.713007  [  512/60000]\n",
      "loss: 0.730299  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.731747 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.710662  [  512/60000]\n",
      "loss: 0.728250  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.729644 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.708343  [  512/60000]\n",
      "loss: 0.726228  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.727562 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.706047  [  512/60000]\n",
      "loss: 0.724216  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.725500 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.703773  [  512/60000]\n",
      "loss: 0.722222  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.723456 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.701519  [  512/60000]\n",
      "loss: 0.720248  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.721431 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.699286  [  512/60000]\n",
      "loss: 0.718287  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.719424 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.697074  [  512/60000]\n",
      "loss: 0.716342  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.717434 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.694883  [  512/60000]\n",
      "loss: 0.714407  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.715459 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.692713  [  512/60000]\n",
      "loss: 0.712488  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.713500 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.690560  [  512/60000]\n",
      "loss: 0.710580  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.711558 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.688423  [  512/60000]\n",
      "loss: 0.708692  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.709631 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.686306  [  512/60000]\n",
      "loss: 0.706814  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.707720 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.684209  [  512/60000]\n",
      "loss: 0.704940  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.705822 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.682125  [  512/60000]\n",
      "loss: 0.703075  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703936 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.680057  [  512/60000]\n",
      "loss: 0.701216  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.702065 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.678002  [  512/60000]\n",
      "loss: 0.699372  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.700209 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.675965  [  512/60000]\n",
      "loss: 0.697547  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.698371 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.673940  [  512/60000]\n",
      "loss: 0.695735  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.696550 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.671927  [  512/60000]\n",
      "loss: 0.693939  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.694746 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.669927  [  512/60000]\n",
      "loss: 0.692155  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.692957 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.667948  [  512/60000]\n",
      "loss: 0.690378  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.691183 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.665983  [  512/60000]\n",
      "loss: 0.688610  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.689423 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.664034  [  512/60000]\n",
      "loss: 0.686853  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.687677 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.662098  [  512/60000]\n",
      "loss: 0.685104  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.685945 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.660177  [  512/60000]\n",
      "loss: 0.683370  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.684228 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.658274  [  512/60000]\n",
      "loss: 0.681649  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.682526 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.656386  [  512/60000]\n",
      "loss: 0.679938  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.680839 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.654514  [  512/60000]\n",
      "loss: 0.678241  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.679165 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.652658  [  512/60000]\n",
      "loss: 0.676555  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.677507 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.650816  [  512/60000]\n",
      "loss: 0.674878  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.675865 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.648989  [  512/60000]\n",
      "loss: 0.673216  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.674239 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.647174  [  512/60000]\n",
      "loss: 0.671566  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.672627 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.645371  [  512/60000]\n",
      "loss: 0.669922  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.671032 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.643582  [  512/60000]\n",
      "loss: 0.668282  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.669451 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.641807  [  512/60000]\n",
      "loss: 0.666660  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.667886 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.640046  [  512/60000]\n",
      "loss: 0.665045  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.666336 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.638299  [  512/60000]\n",
      "loss: 0.663446  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.664801 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.636564  [  512/60000]\n",
      "loss: 0.661854  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.663280 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.634843  [  512/60000]\n",
      "loss: 0.660272  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.661774 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.633135  [  512/60000]\n",
      "loss: 0.658701  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.660282 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.631440  [  512/60000]\n",
      "loss: 0.657142  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.658804 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.629757  [  512/60000]\n",
      "loss: 0.655594  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.657341 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.628086  [  512/60000]\n",
      "loss: 0.654054  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.655893 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.626428  [  512/60000]\n",
      "loss: 0.652525  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.654457 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.624783  [  512/60000]\n",
      "loss: 0.651005  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.653037 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.623154  [  512/60000]\n",
      "loss: 0.649499  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.651630 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.621534  [  512/60000]\n",
      "loss: 0.647993  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.650236 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.619928  [  512/60000]\n",
      "loss: 0.646508  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.648856 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.618332  [  512/60000]\n",
      "loss: 0.645029  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.647491 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.616747  [  512/60000]\n",
      "loss: 0.643563  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.646139 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.615176  [  512/60000]\n",
      "loss: 0.642109  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.644800 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.613615  [  512/60000]\n",
      "loss: 0.640665  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.643474 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.612069  [  512/60000]\n",
      "loss: 0.639229  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.642161 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.610531  [  512/60000]\n",
      "loss: 0.637805  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.640861 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.609007  [  512/60000]\n",
      "loss: 0.636395  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.639574 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.607494  [  512/60000]\n",
      "loss: 0.634996  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.638302 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.605993  [  512/60000]\n",
      "loss: 0.633607  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.637042 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.604504  [  512/60000]\n",
      "loss: 0.632229  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.635796 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.603027  [  512/60000]\n",
      "loss: 0.630860  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.634561 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.601563  [  512/60000]\n",
      "loss: 0.629502  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.633341 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.600113  [  512/60000]\n",
      "loss: 0.628154  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.632134 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.598672  [  512/60000]\n",
      "loss: 0.626812  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.630939 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.597240  [  512/60000]\n",
      "loss: 0.625481  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.629757 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.595819  [  512/60000]\n",
      "loss: 0.624158  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.628586 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.594408  [  512/60000]\n",
      "loss: 0.622846  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.627428 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.593010  [  512/60000]\n",
      "loss: 0.621547  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.626281 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.591623  [  512/60000]\n",
      "loss: 0.620262  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.625146 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.590247  [  512/60000]\n",
      "loss: 0.618987  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.624023 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.588883  [  512/60000]\n",
      "loss: 0.617724  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.622913 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.587532  [  512/60000]\n",
      "loss: 0.616470  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.621814 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.586191  [  512/60000]\n",
      "loss: 0.615224  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.620727 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.584862  [  512/60000]\n",
      "loss: 0.613988  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.619650 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.583547  [  512/60000]\n",
      "loss: 0.612764  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.618585 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.582240  [  512/60000]\n",
      "loss: 0.611545  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.617530 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4dce01a0-58b1-449b-9593-c3e65a34e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'my.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2d0c91b9-9134-4b31-a3ec-b9101fd2f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten(start_dim=1, end_dim=-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MyNeuralNetwork()\n",
    "model2.load_state_dict(torch.load('my.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "de56d608-6958-4810-ac29-6ed10c817857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1010c066-43ad-49ef-b531-d8e0d74bf81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.580942  [  512/60000]\n",
      "loss: 0.610337  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.616486 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.579650  [  512/60000]\n",
      "loss: 0.609138  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.615452 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.578368  [  512/60000]\n",
      "loss: 0.607948  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.614429 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.577097  [  512/60000]\n",
      "loss: 0.606770  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.613417 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.575834  [  512/60000]\n",
      "loss: 0.605596  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.612415 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.574580  [  512/60000]\n",
      "loss: 0.604429  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.611423 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.573338  [  512/60000]\n",
      "loss: 0.603274  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.610441 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.572103  [  512/60000]\n",
      "loss: 0.602128  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.609468 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.570878  [  512/60000]\n",
      "loss: 0.600994  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.608504 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.569664  [  512/60000]\n",
      "loss: 0.599869  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.607549 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.568459  [  512/60000]\n",
      "loss: 0.598754  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.606603 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.567263  [  512/60000]\n",
      "loss: 0.597647  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.605668 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.566073  [  512/60000]\n",
      "loss: 0.596548  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.604742 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.564897  [  512/60000]\n",
      "loss: 0.595457  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.603823 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.563730  [  512/60000]\n",
      "loss: 0.594379  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.602914 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.562568  [  512/60000]\n",
      "loss: 0.593308  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.602013 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.561413  [  512/60000]\n",
      "loss: 0.592245  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.601121 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.560268  [  512/60000]\n",
      "loss: 0.591190  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.600237 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.559136  [  512/60000]\n",
      "loss: 0.590145  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.599360 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.558007  [  512/60000]\n",
      "loss: 0.589106  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.598490 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.556885  [  512/60000]\n",
      "loss: 0.588073  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.597629 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.555767  [  512/60000]\n",
      "loss: 0.587051  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.596774 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.554658  [  512/60000]\n",
      "loss: 0.586036  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.595927 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.553558  [  512/60000]\n",
      "loss: 0.585027  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.595087 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.552465  [  512/60000]\n",
      "loss: 0.584028  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.594253 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.551378  [  512/60000]\n",
      "loss: 0.583032  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.593427 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.550297  [  512/60000]\n",
      "loss: 0.582042  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.592607 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.549224  [  512/60000]\n",
      "loss: 0.581057  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.591793 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.548157  [  512/60000]\n",
      "loss: 0.580078  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.590985 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.547093  [  512/60000]\n",
      "loss: 0.579102  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.590182 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.546032  [  512/60000]\n",
      "loss: 0.578140  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.589384 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.544980  [  512/60000]\n",
      "loss: 0.577193  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.588592 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.543936  [  512/60000]\n",
      "loss: 0.576249  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.587806 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.542897  [  512/60000]\n",
      "loss: 0.575321  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.587024 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.541859  [  512/60000]\n",
      "loss: 0.574396  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.586247 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.540822  [  512/60000]\n",
      "loss: 0.573450  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.585472 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.539791  [  512/60000]\n",
      "loss: 0.572508  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.584705 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.538771  [  512/60000]\n",
      "loss: 0.571576  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.583942 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.537767  [  512/60000]\n",
      "loss: 0.570645  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.583189 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.536777  [  512/60000]\n",
      "loss: 0.569727  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.582444 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.535787  [  512/60000]\n",
      "loss: 0.568813  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.581706 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.534796  [  512/60000]\n",
      "loss: 0.567908  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.580974 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.533805  [  512/60000]\n",
      "loss: 0.567008  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.580248 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.532821  [  512/60000]\n",
      "loss: 0.566115  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.579529 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.531843  [  512/60000]\n",
      "loss: 0.565231  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.578815 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.530870  [  512/60000]\n",
      "loss: 0.564348  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.578105 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.529898  [  512/60000]\n",
      "loss: 0.563468  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.577400 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.528933  [  512/60000]\n",
      "loss: 0.562595  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.576699 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.527974  [  512/60000]\n",
      "loss: 0.561728  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.576003 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.527021  [  512/60000]\n",
      "loss: 0.560863  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.575313 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.526070  [  512/60000]\n",
      "loss: 0.560006  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.574626 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.525125  [  512/60000]\n",
      "loss: 0.559154  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.573943 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.524186  [  512/60000]\n",
      "loss: 0.558309  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.573266 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.523251  [  512/60000]\n",
      "loss: 0.557464  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.572592 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.522320  [  512/60000]\n",
      "loss: 0.556629  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.571922 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.521390  [  512/60000]\n",
      "loss: 0.555793  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.571256 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.520466  [  512/60000]\n",
      "loss: 0.554966  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.570594 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.519548  [  512/60000]\n",
      "loss: 0.554146  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.569936 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.518635  [  512/60000]\n",
      "loss: 0.553324  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.569280 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.517727  [  512/60000]\n",
      "loss: 0.552508  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.568629 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.516821  [  512/60000]\n",
      "loss: 0.551694  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.567981 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.515920  [  512/60000]\n",
      "loss: 0.550884  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.567337 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.515020  [  512/60000]\n",
      "loss: 0.550077  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.566695 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.514122  [  512/60000]\n",
      "loss: 0.549271  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.566057 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.513230  [  512/60000]\n",
      "loss: 0.548471  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.565422 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.512340  [  512/60000]\n",
      "loss: 0.547671  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.564791 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.511454  [  512/60000]\n",
      "loss: 0.546879  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.564163 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.510565  [  512/60000]\n",
      "loss: 0.546093  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.563538 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.509683  [  512/60000]\n",
      "loss: 0.545311  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.562915 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.508805  [  512/60000]\n",
      "loss: 0.544530  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.562295 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.507932  [  512/60000]\n",
      "loss: 0.543755  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.561676 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.507065  [  512/60000]\n",
      "loss: 0.542986  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.561060 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.506201  [  512/60000]\n",
      "loss: 0.542223  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.560447 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.505341  [  512/60000]\n",
      "loss: 0.541464  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.559837 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.504484  [  512/60000]\n",
      "loss: 0.540711  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.559230 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.503629  [  512/60000]\n",
      "loss: 0.539969  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.558625 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.502780  [  512/60000]\n",
      "loss: 0.539228  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.558022 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.501932  [  512/60000]\n",
      "loss: 0.538490  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.557424 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.501090  [  512/60000]\n",
      "loss: 0.537751  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.556826 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.500250  [  512/60000]\n",
      "loss: 0.537020  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.556233 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.499414  [  512/60000]\n",
      "loss: 0.536296  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.555642 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.498583  [  512/60000]\n",
      "loss: 0.535576  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.555055 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.497751  [  512/60000]\n",
      "loss: 0.534858  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.554471 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.496923  [  512/60000]\n",
      "loss: 0.534144  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.553890 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.496096  [  512/60000]\n",
      "loss: 0.533435  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.553313 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.495276  [  512/60000]\n",
      "loss: 0.532730  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.552740 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.494456  [  512/60000]\n",
      "loss: 0.532034  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.552172 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.493643  [  512/60000]\n",
      "loss: 0.531334  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.551607 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.492836  [  512/60000]\n",
      "loss: 0.530636  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.551044 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.492029  [  512/60000]\n",
      "loss: 0.529947  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.550485 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.491224  [  512/60000]\n",
      "loss: 0.529263  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.549928 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.490425  [  512/60000]\n",
      "loss: 0.528582  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.549376 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.489629  [  512/60000]\n",
      "loss: 0.527901  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.548825 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.488836  [  512/60000]\n",
      "loss: 0.527226  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.548277 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.488046  [  512/60000]\n",
      "loss: 0.526555  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.547731 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.487260  [  512/60000]\n",
      "loss: 0.525888  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.547189 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.486479  [  512/60000]\n",
      "loss: 0.525225  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.546650 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.485701  [  512/60000]\n",
      "loss: 0.524567  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.546113 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.484927  [  512/60000]\n",
      "loss: 0.523905  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.545578 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.484156  [  512/60000]\n",
      "loss: 0.523245  [51712/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.545045 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model2, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model2, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff2eda-4fec-4087-ad5e-ad0d8c2f7c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
